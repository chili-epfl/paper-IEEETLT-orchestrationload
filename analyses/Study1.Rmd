---
title: "Study1"
author: "lprisan"
date: "20 de marzo de 2016"
output: html_document
---

(add some contextual info from the paper)

As a first step, we download the [Dataset for Study 1 (Multi-tabletop sessions in the lab, primary school students)](https://zenodo.org/record/16515), and preprocess it. The preprocessing basically consists on aggregating the four load-related eyetracking metrics (pupil diameter mean, pupil diameter variation, saccade speed and number of fixations >500ms) into 10-second episodes, using a rolling window with 5-second slide between windows (see ```./lib/aggregateEpisodeData.R``` and ```./lib/rollingWindows.R``` files for details). Then, that aggregated data is merged with the video codes generated by researchers, regarding the social level, teacher activity and gaze focus (during the 10-second episodes where all eyetracking metrics agreed on high/low values).

```{r, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
# We load the useful scripts and packages needed throughout the report
source("./lib/rollingWindows.R")
source("./lib/loadIndex.R")
source("./lib/extremeLoadExtraction.R")
source("./lib/aggregateEpisodeData.R")
source("./lib/multiplot.R")
source("./lib/outliers.R")

# Create the directory structure onto which download the data
rootdir <- getwd()
datadir <- paste(rootdir,"data","study1",sep=.Platform$file.sep)
if(!dir.exists(datadir)) dir.create(datadir, recursive=T)
setwd(datadir)

# Download the data
if(!file.exists("JDC2014-VideoCodingData.zip") || !file.exists("JDC2014-EyetrackingData.zip")){
    download.file("https://zenodo.org/record/16515/files/JDC2014-VideoCodingData.zip", destfile="JDC2014-VideoCodingData.zip", method="curl")
    unzip("JDC2014-VideoCodingData.zip")
    download.file("https://zenodo.org/record/16515/files/JDC2014-EyetrackingData.zip", destfile="JDC2014-EyetrackingData.zip", method="curl")
    unzip("JDC2014-EyetrackingData.zip")
}

# We do the preprocessing, which will generate a Rda file with the 10s
# window data, and will return the name of the file
sessions <- c("JDC2014-Session1-eyetracking","JDC2014-Session2-eyetracking","JDC2014-Session3-eyetracking")
totaldata <- data.frame()
cleandatafile <- "study1ProcessedData.Rda"
if(!file.exists(cleandatafile)){
    data <- aggregateEpisodeData(sessions, datadir=datadir, initendtimes=NULL, SEPARATOR=",") # For this study the raw data is comma-separated!
    data <- data[,c(1:5,12)] # We select only the load-related metrics
    # We load and add the video coding data with the social, activity and main gaze focus dimensions
    videocodes <- data.frame()
    sessionsvid <- c("JDC2014-Session1","JDC2014-Session2","JDC2014-Session3") # For some reason, the filenames for the videocoding are not consistent with the previous session labels
    for(session in sessionsvid){
        sessioncodes <- read.csv(paste(session,"-videocoding.csv",sep = ""), sep=",")
        if(nrow(videocodes)==0) videocodes <- sessioncodes
        else videocodes <- rbind(videocodes,sessioncodes)
    }
    videocodes$session <- videocodes$Session
    totaldata <- merge(data,videocodes,by=c("session","time"),all=T)
    save(totaldata, file=cleandatafile)
}else{
  totaldata <- get(load(file=cleandatafile))
}

```


## Calculate the PCA Load index

... from the eyetracking metrics, normalized by the value of the first three 10-s windows, to account for variations in the day's data (e.g., due to tiredness, etc.)

```{r, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
#names(totaldata)
# We crop outliers in the saccade speed. Some times they have very extreme outliers (measurement errors) that tend to throw off the PCA calculations
#countOutliers(totaldata$value.Sac,coef = 5, method="iqr") # 5xinter-quartile range
totaldata$value.Sac <- replaceOutliers(totaldata$value.Sac,valueNA = F,coef = 5, method="iqr")

loaddata <- calculateCoarseFineLoadIndex(totaldata,3:6,normalize=T,stablenorm = 3) # We ensure that the values are normalized for those of the first windows in which little load is assumed (just putting the eyetracker on)
#names(loaddata)

library(FactoMineR)

#Overall for the four sessions
res.pca.norm = PCA(loaddata[, c(13,16,19,22)], scale.unit=TRUE, ncp=5, graph=F)
plot.PCA(res.pca.norm, axes=c(1, 2), choix="var", title="PCA, Normalized data, dims 1/2")
loaddata$PCALoad = res.pca.norm$ind$coord[,1]


# We plot the loads for each session, along with some smoothing
for(session in sessions){
    sessiondata <- loaddata[loaddata$session==session,]
    p1 <- ggplot(sessiondata, aes(x=time/60000, y=PCALoad, col=PCALoad)) + 
            ggtitle(paste("PCA Load Index ",session,sep="")) + 
            geom_line(size=1) + stat_smooth(method="loess",span=0.1,se=F) +
            #theme(axis.text.x = element_text(size=18),plot.title=element_text(size=20, face="bold"),axis.title=element_text(size=18),panel.background = element_rect(fill = 'white')) +
            theme(panel.background = element_rect(fill = 'white')) +
            scale_color_gradient(low="green",high="red")
    print(p1)

}


```


## Orchestration load patterns (linear model)


To get an idea of how the PCA load index is related to the different kinds of classroom episodes, in terms of the orchestration dimensions coded by a human researcher (teacher activity, social plane of interaction and main focus of the teacher gaze), we can produce a linear model:

```{r, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}


behdata <- loaddata[!is.na(loaddata$Activity) & !is.na(loaddata$Social) & !is.na(loaddata$Focus),]

#ggplot(behdata,aes(x=PCALoad))+geom_density()
# The distribution of coded values goes continuouosly along the dimension, so we do a linear model

# Optional: delete the factor values with too few occurrences?
behdata <- behdata[behdata$Focus!="TEA",]
behdata <- behdata[behdata$Focus!="W",]
behdata <- behdata[behdata$Focus!="M",]
behdata <- behdata[behdata$Focus!="RES",]
behdata$Focus <- factor(behdata$Focus)

lm1 <- lm(PCALoad ~ Activity + Social + Focus, data = behdata)
summary(lm1)
anova(lm1, test="Chisq")
```

We observe that, for these episodes coded, a simple linear model of the three orchestration dimensions explains 36% of the variance in PCA Load Index. In this case we see that the focus of the gaze is a significant contributor to the predictor model, most notably through the TABletop episodes, which are in general less load. The intercept (EXPlanations, looking at students BAKs, in a CLSroom social level) represent a good predictor for higher loads.

<!--TODO: Remove the appendix?-->
## Appendix: Using Factor analysis instead of PCA load index

... results are largely the same, even if some of the coefficient estimations are different

```{r, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}

library(psych)
X <- loaddata[, c(13,16,19,22)]
N <- nrow(loaddata[, c(13,16,19,22)])
corMat <- cor(X)
faPC  <- fa(r=corMat, nfactors=1, n.obs=N, rotate="varimax")
bartlett <- factor.scores(x=X, f=faPC, method="Bartlett")
loaddata$FALoad <- bartlett$scores[,1]
# factor.plot(faPC, cut=0.3)
# fa.diagram(faPC)
# fa.parallel(loaddata[, c(13,16,19,22)])
# vss(loaddata[, c(13,16,19,22)], n.obs=nrow(loaddata[, c(13,16,19,22)]), rotate="varimax")

# We plot the loads for each session, along with some smoothing -- values are different, but smoothing is pretty similar between PCA and FA
# for(session in sessions){
#     sessiondata <- loaddata[loaddata$session==session,]
# 
#     p1 <- ggplot(sessiondata, aes(x=time/60000, y=PCALoad, col=PCALoad)) + 
#             ggtitle(paste("PCA Load Index ",session,sep="")) + 
#             geom_line(size=1) + stat_smooth(method="loess",span=0.1,se=F) +
#             #theme(axis.text.x = element_text(size=18),plot.title=element_text(size=20, face="bold"),axis.title=element_text(size=18),panel.background = element_rect(fill = 'white')) +
#             theme(panel.background = element_rect(fill = 'white')) +
#             scale_color_gradient(low="green",high="red")
#     print(p1)
# 
#     
#     p1b <- ggplot(sessiondata, aes(x=time/60000, y=FALoad, col=FALoad)) + 
#             ggtitle(paste("FA Load Index ",session,sep="")) + 
#             geom_line(size=1) + stat_smooth(method="loess",span=0.1,se=F) +
#             #theme(axis.text.x = element_text(size=18),plot.title=element_text(size=20, face="bold"),axis.title=element_text(size=18),panel.background = element_rect(fill = 'white')) +
#             theme(panel.background = element_rect(fill = 'white')) +
#             scale_color_gradient(low="green",high="red")
#     print(p1b)
# 
# }

behdata <- loaddata[!is.na(loaddata$Activity) & !is.na(loaddata$Social) & !is.na(loaddata$Focus),]

#ggplot(behdata,aes(x=PCALoad))+geom_density()
# The distribution of coded values goes continuouosly along the dimension, so we do a linear model

# Optional: delete the factor values with too few occurrences?
behdata <- behdata[behdata$Focus!="TEA",]
behdata <- behdata[behdata$Focus!="W",]
behdata <- behdata[behdata$Focus!="M",]
behdata <- behdata[behdata$Focus!="RES",]
behdata$Focus <- factor(behdata$Focus)

lm2 <- lm(FALoad ~ Activity + Social + Focus, data = behdata)
summary(lm2)
anova(lm2, test="Chisq")
```





***


```{r, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
# We go back to the root directory for the next study's scripts
setwd(rootdir)
```