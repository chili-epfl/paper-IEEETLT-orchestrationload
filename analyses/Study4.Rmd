---
title: "Study4"
author: "lprisan"
date: "April 13, 2016"
output: html_document
---

(add some contextual info from the paper)

As a first step, we download the [dataset for Study 4]() (which has been published in Zenodo), and preprocess it. The preprocessing again consists on aggregating the four load-related eyetracking metrics into 10-second episodes, using a rolling window with 5-second slide between windows. Then, that aggregated data is merged with the video codes generated by researchers, regarding the social level, teacher activity and gaze focus (during the 10-second episodes where all eyetracking metrics agreed on high/low values). 


```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(XML)
library(reshape)
library(rjson)
library(plyr)
# We load the useful scripts and packages needed throughout the report
source("./lib/rollingWindows.R")
source("./lib/loadIndex.R")
source("./lib/extremeLoadExtraction.R")
source("./lib/aggregateEpisodeData.R")
source("./lib/multiplot.R")
source("./lib/outliers.R")
source("./lib/preprocessVideoCoding.R")
source("./lib/predictor.R")

# Create the directory structure onto which download the data
rootdir <- getwd()
datadir <- paste(rootdir,"data","study4",sep=.Platform$file.sep)
if(!dir.exists(datadir)) dir.create(datadir, recursive=T)
setwd(datadir)

# TODO: Add the download commands for each file once it has been uploaded to zenodo
# We download and uncompress the data we need -- the eyetracking and videocoding data
# if(!file.exists("JDC2015-CodingData.zip") ||
#    !file.exists("JDC2015-EyetrackingData.zip")){ 
# 
#     #    download.file("https://zenodo.org/record/16551/files/ISL2014BASELINE-QuestionnaireData.zip", destfile="ISL2014BASELINE-QuestionnaireData.zip", method="curl")
#     unzip("JDC2015-CodingData.zip")
#     unzip("JDC2015-EyetrackingData.zip")
# } 

# Now we have the raw data files uncompressed in the data/study4 folder

sessions <-  c("JDC2015-Session1","JDC2015-Session2","JDC2015-Session3","JDC2015-Session4")

totaldata <- data.frame()
cleandatafile <- "study4ProcessedData.Rda"
if(!file.exists(cleandatafile)){

    annotationsData <- preprocessVideoCoding(datadir,sessions)
    initendtimes <- annotationsData[annotationsData$tier=="Recording" 
                                    & annotationsData$annotation == "Recording",
                                    c("start","end","session")]
    
    eyedata <- aggregateEpisodeData(sessions, datadir=datadir, initendtimes=initendtimes, SEPARATOR=";") # For this study the raw data is semicolon-separated, at least the fixation/saccades!
    eyedata <- eyedata[,c(1:5,12)] # We select only the load-related metrics

    # We calculate the video coding values for the same windows
    videocodedata <- aggregateVideoCodingData(sessions, datadir=datadir, initendtimes=initendtimes)

    totaldata <- merge(eyedata,videocodedata,by=c("session","time"),all=T)
    save(totaldata, file=cleandatafile)

}else{
  totaldata <- get(load(file=cleandatafile))
}

# Remove missing values
totaldata <- totaldata[!is.na(totaldata$value.Sac),]
# We crop outliers in the saccade speed. They tend to throw off the HMM and PCA calculations
#countOutliers(totaldata$value.Sac,coef = 5, method="iqr") # 5xinter-quartile range
totaldata$value.Sac <- replaceOutliers(totaldata$value.Sac,valueNA = F,coef = 5, method="iqr")

loaddata <- calculateCoarseFineLoadIndex(totaldata,3:6,normalize=T,stablenorm = 3) # We ensure that the values are normalized for those of the first window in which little load is assumed (just putting the eyetracker on)
#str(loaddata)
#names(loaddata)


# We assume that the teacher load in the parts where he received help (C1, during sessions 1 and 4) is lower than in those equivalent parts of the session where he did not receive help (C2, during sessions 2 and 3)
# The rest of episodes, we make no assumption about the teacher load (NA)
loaddata$TeacherLoad <- ifelse(loaddata$value.Experimental=="C2",1,ifelse(loaddata$value.Experimental=="C1",0,NA))

loaddata$value.Activity <- factor(loaddata$value.Activity)
loaddata$value.Social <- factor(loaddata$value.Social)
loaddata$value.Focus <- factor(loaddata$value.Focus)


```

Aside from those variables in Study 1, we have another "assumed ground truth" variable (`TeacherLoad`), which associates the teacher load in the **parts where teacher received help** (C1, during sessions 1 and 4) is *lower* than in those equivalent parts of the session where he **did not receive help** (C2, during sessions 2 and 3). In the rest of episodes, we make no assumption about the teacher load (NA).


## Calculate the PCA Load index

... from the eyetracking metrics, normalized by the value of the first three 10-s windows, to account for variations in the day's data (e.g., due to tiredness, etc.)

```{r, message=FALSE, warning=FALSE, echo=FALSE}

# We calculate the PCA load index (separately for the expert and novice teacher)
library(FactoMineR)

res.pca.norm = PCA(loaddata[, c(11,14,17,20)], scale.unit=TRUE, ncp=5, graph=F)
plot.PCA(res.pca.norm, axes=c(1, 2), choix="var", title="PCA All sessions, Normalized data, dims 1/2")
loaddata$PCALoad = res.pca.norm$ind$coord[,1]

# We plot the loads for each session, along with some smoothing
for(session in sessions){
    sessiondata <- loaddata[loaddata$session==session,]
    p1 <- ggplot(sessiondata, aes(x=time/60000, y=PCALoad, col=PCALoad)) + 
            ggtitle(paste("PCA Load Index ",session,sep="")) + 
            geom_line(size=1) + stat_smooth(method="loess",span=0.1,se=F) +
            #theme(axis.text.x = element_text(size=18),plot.title=element_text(size=20, face="bold"),axis.title=element_text(size=18),panel.background = element_rect(fill = 'white')) +
            theme(panel.background = element_rect(fill = 'white')) +
            scale_color_gradient(low="green",high="red")
    print(p1)

}

```



## Validation of the PCA Load measure

That is: Can this PCA Load index help us distinguish between episodes where the teacher had help and those were teacher did not have help? Or, put another way, is the PCA Load index different, once we remove the effect of the other process variables (teacher activity, social plane of interaction, main focus of the gaze)


```{r, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}

behdata <- loaddata[!is.na(loaddata$value.Activity) & !is.na(loaddata$value.Social) & !is.na(loaddata$value.Focus),]
# table(behdata$TeacherLoad,behdata$value.Activity)
# table(behdata$TeacherLoad,behdata$value.Social)
# table(behdata$TeacherLoad,behdata$value.Focus)

# We remove the levels for which we do not have enough data
behdata <- behdata[behdata$value.Activity!="EXP",]
behdata <- behdata[behdata$value.Activity!="OFF",]
behdata <- behdata[behdata$value.Activity!="TEC",]
behdata$value.Activity <- factor(behdata$value.Activity)
behdata <- behdata[behdata$value.Focus!="WHI",]
behdata <- behdata[behdata$value.Focus!="TCOMP",]
behdata <- behdata[behdata$value.Focus!="TD",]
behdata <- behdata[behdata$value.Focus!="TEA",]
behdata <- behdata[behdata$value.Focus!="TPAP",]
behdata$value.Focus <- factor(behdata$value.Focus)

beh <- behdata[!is.na(behdata$TeacherLoad),]
```

### Error in a linear model

```{r, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
lmbase <- lm(PCALoad ~ value.Activity + value.Social + value.Focus, data = beh)
summary(lmbase)
anova(lmbase)

beh$errorlm <- getModelError(beh,c("value.Activity","value.Social","value.Focus"), lmbase, "PCALoad")
t.test(beh[beh$TeacherLoad==0,"errorlm"],beh[beh$TeacherLoad==1,"errorlm"])
ggplot(beh, aes(x=errorlm, col=factor(TeacherLoad)))+geom_density()

```


### Logistic regression model

To do so, we train a logistic regression model that tries to predict the "had helper condition" on the basis of the PCA Load index and the orchestration process variables coded by a human:


```{r, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}


# We train the logistic regression model
lr1 <- glm(TeacherLoad ~ value.Activity + value.Social + value.Focus + PCALoad, family=binomial(link='logit'), data=beh) # The PCA load goes in the wrong direction!!!!??? recheck the data...
summary(lr1)
anova(lr1, test="Chisq")
library(caret)
set.seed(1)
Train <- createDataPartition(beh$TeacherLoad, p=0.7, list=FALSE)
training <- beh[ Train, ]
testing <- beh[ -Train, ]
#mod_fit <- train(TeacherLoad ~ value.Activity + value.Social + value.Focus + PCALoad,  
#                 data=training, method="glm", family="binomial")

mod_fit <- glm(TeacherLoad ~ value.Activity + value.Social + value.Focus + PCALoad, 
                   data=training, family="binomial")
mod_fit_base <- glm(TeacherLoad ~ value.Activity + value.Social + value.Focus, 
                   data=training, family="binomial")

# Compare the model with PCA load with the model without it
print("ANOVA of logistic regression models with and without the PCA Load as a variable")
anova(mod_fit_base, mod_fit, test ="Chisq")

# McFadden's pseudo-Rsquared, to get an idea of the proportion of variance explained
library(pscl)
print("McFadden's pseudo Rsquared, base model WITHOUT PCA Load")
pR2(mod_fit_base)["McFadden"]  # look for 'McFadden', 0.03
print("McFadden's pseudo Rsquared, model WITH PCA Load")
pR2(mod_fit)["McFadden"]  # look for 'McFadden', 0.28

# Wald test for individual predictors
library(survey)
#regTermTest(mod_fit, "value.Activity") 
#regTermTest(mod_fit, "value.Social") # p=0.006, removing this variable would harm substantially the fit of the model
#regTermTest(mod_fit, "value.Focus") # p=0.03, removing this variable would harm substantially the fit of the model
print("Wald test: would removing the PCA Load harm the model fit")
regTermTest(mod_fit, "PCALoad") # p=2e-13, removing this variable would harm substantially the fit of the model

# Variable importance
print("Variable importance")
varImp(mod_fit) # PCALoad is the most important variable

# Predictions and accuracy of the model (as a curiosity)
# ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
# mod_fit <- train(TeacherLoad ~ value.Activity + value.Social + value.Focus + PCALoad,  data=training,
#                  method="glm", family="binomial", trControl = ctrl, tuneLength = 5)
# mod_base <- train(TeacherLoad ~ value.Activity + value.Social + value.Focus,  data=training,
#                  method="glm", family="binomial", trControl = ctrl, tuneLength = 5)
# pred = predict(mod_fit, newdata=testing)
# predbase = predict(mod_base, newdata=testing)
# eval = data.frame(cutoff=seq(from=0, to=1, length.out=100))
# eval$base = NA
# eval$model = NA
# for(i in 1:nrow(eval)){
#     eval[i,"model"] <- confusionMatrix(data=ifelse(pred>eval$cutoff[i],1,0), testing$TeacherLoad)$overall["Accuracy"]
#     eval[i,"base"] <- confusionMatrix(data=ifelse(predbase>eval$cutoff[i],1,0), testing$TeacherLoad)$overall["Accuracy"]
# }
# ggplot(eval, aes(x=cutoff, y=model))+geom_line()+geom_line(aes(y=base), col="red")
# max(eval$model) # Max accuracy, 78%
# max(eval$base) # Max accuracy, 65%

```

We observe that not only the PCALoad is a significant predictor of the assumed ground truth variable, it is actually the most important variable in the model, even more so than the other process variables (activity, social & focus).

TODO: Check whether the sign of the influence of PCA Load is correct in all studies!


TODO: complete the rest of sections!



## Appendix: Using Factor analysis instead of PCA load index

... results are ...

```{r, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}

library(psych)

#All data
X <- loaddata[, c(11,14,17,20)]
N <- nrow(loaddata[, c(11,14,17,20)])
corMat <- cor(X)
faPC  <- fa(r=corMat, nfactors=1, n.obs=N, rotate="varimax")
bartlett <- factor.scores(x=X, f=faPC, method="Bartlett")
loaddata$FALoad <- bartlett$scores[,1]
# factor.plot(faPC, cut=0.3)
# fa.diagram(faPC)
# fa.parallel(X)
# vss(X, n.obs=N, rotate="varimax")

behdata <- loaddata[!is.na(loaddata$value.Activity) & !is.na(loaddata$value.Social) & !is.na(loaddata$value.Focus),]
# table(behdata$TeacherLoad,behdata$value.Activity)
# table(behdata$TeacherLoad,behdata$value.Social)
# table(behdata$TeacherLoad,behdata$value.Focus)

# We remove the levels for which we do not have enough data
behdata <- behdata[behdata$value.Activity!="EXP",]
behdata <- behdata[behdata$value.Activity!="OFF",]
behdata <- behdata[behdata$value.Activity!="TEC",]
behdata$value.Activity <- factor(behdata$value.Activity)
behdata <- behdata[behdata$value.Focus!="WHI",]
behdata <- behdata[behdata$value.Focus!="TCOMP",]
behdata <- behdata[behdata$value.Focus!="TD",]
behdata <- behdata[behdata$value.Focus!="TEA",]
behdata <- behdata[behdata$value.Focus!="TPAP",]
behdata$value.Focus <- factor(behdata$value.Focus)

beh <- behdata[!is.na(behdata$TeacherLoad),]

# Validation of FALoad using logistic regression model
lr1 <- glm(TeacherLoad ~ value.Activity + value.Social + value.Focus + FALoad, family=binomial(link='logit'), data=beh)
summary(lr1)
anova(lr1, test="Chisq")


```





